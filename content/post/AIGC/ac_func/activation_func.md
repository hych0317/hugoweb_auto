+++
title = '激活函数'
date = 2024-10-20T21:01:22+08:00
draft = false
categories = ["AIGC"]
+++

## 激活函数
引入激活函数是为了增加神经网络模型的非线性。若没有激活函数的每层都相当于矩阵相乘。没有激活函数的神经网络叠加了若干层之后，还是一个线性变换，与单层感知机无异。
![激活函数示意图](post/AIGC/ac_func/main.png)

### 分类
* 饱和激活函数： sigmoid、 tanh...
* 非饱和激活函数:  ReLU 、Leaky Relu  、ELU、PReLU、RReLU...

当x趋向于正无穷时，函数的导数趋近于0，此时称为右饱和。
当x趋向于负无穷时，函数的导数趋近于0，此时称为左饱和。
>当一个函数既满足右饱和，又满足左饱和，则称为饱和函数，否则称为非饱和函数。

非饱和激活函数的优势在于两点：
1. 非饱和激活函数能解决深度神经网络（层数非常多）带来的梯度消失问题

2. 使用非饱和激活函数能加快收敛速度

PS:
* 梯度饱和导致梯度消失：梯度饱和是梯度消失的主要原因之一。当激活函数的输出趋于饱和值时，其导数很小，导致梯度变小。随着网络层数的增加，这种小梯度不断积累，从而导致梯度消失。

* 梯度消失与梯度爆炸：这两者都是在深度网络中累乘梯度过程中产生的，但它们是两个极端情况。梯度消失是梯度在传播过程中不断缩小，而梯度爆炸是梯度不断放大。

### 常见激活函数

#### sigmoid函数
$$\sigma(x) = \frac{1}{1+e^{-x}}$$
其导数为：
$$\sigma'(x) =  \frac{\exp(-x)}{(1+e^{-x})^2} = \sigma(x)(1-\sigma(x))$$
图像：  
![sigmoid函数图像](post/AIGC/ac_func/sigmoid.png)  

优点：
1. Sigmoid 函数的输出范围是 0 到 1。预测值非常接近0/1，可以用于表示二分类的类别或者用于表示置信度。
2. 梯度平滑，便于求导，也防止模型训练过程中出现突变的梯度

缺点：
1. Sigmoid 函数的输出不是以 0 为中心的，可能导致模型输出的均值偏离 0。
2. Sigmoid 函数在计算过程中容易出现梯度消失的问题。
3. Sigmoid 函数需要计算指数函数，计算量大，效率低。

#### softmax函数（归一化指数函数）
$$softmax(x_i) = \frac{\exp(x_i)}{\sum_{j}\exp(x_j)}$$

特点：
1. Softmax函数常用于**多类分类问题**的输出层激活函数。
对于长度为K的任意实向量，Softmax函数可以将其压缩为长度为K，值在[0,1]范围内，并且向量中元素的总和为1的实向量（即概率分布向量）。其值反映了该向量中各个元素的概率。

输出层使用例：  
![softmax函数图像](post/AIGC/ac_func/softmax_exp.png)  
#### tanh函数
$$tanh(x) = \frac{\exp(x)-\exp(-x)}{\exp(x)+\exp(-x)}$$
其导数为：
$$tanh'(x) = 1-tanh^2(x)$$
实际上，tanh函数就是将sigmoid函数的输出拉伸到-1到1之间。  
\[tanh(x) = 2\sigma(2x)-1\]

图像：  
![tanh函数图像](post/AIGC/ac_func/tanh.png)  

优点：
1. 函数以 0 为中心，输出范围是 -1 到 1。
2. tanh 函数的导数在 0 处梯度为1，因此可以减轻梯度饱和问题。

缺点：
1. 仍存在梯度饱和的问题。
2. 仍是指数运算。

#### softsign函数（更平滑的tanh函数）
$$softsign(x) = \frac{x}{1+|x|}$$
其导数为：
$$softsign'(x) = \frac{1}{1+|x|^2}$$

图像：  
![softsign函数图像](post/AIGC/ac_func/softsign.png)  

优点：
1. 曲线更平坦、梯度下降更慢，表明它可以更高效地学习
2. 可以更好的减轻梯度饱和问题。

缺点：
1. 计算更麻烦

#### ReLU函数（线性整流函数）
$$ReLU(x) = max(0,x)$$

图像：  
![ReLU函数图像](post/AIGC/ac_func/relu.png)  

优点：
1. 计算简单，速度快。
2. 输入为正时，不存在梯度饱和。

缺点：
1. 输出不是以 0 为中心，可能导致模型输出的均值偏离0。
2. Dead ReLU问题：当神经元的输入为负数，则该神经元的梯度为0，输出恒为0，不再对输入数据有所响应，导致其后参数不再更新。

**需要注意的是，虽然Leaky ReLU和ELU函数都能解决ReLU函数的死亡问题，但实践中并未表明他们比ReLU函数效果更好。**

#### softplus函数（ReLU的平滑版本）
$$softplus(x) = \ln(1+e^x)$$

图像：  
![softplus函数图像](post/AIGC/ac_func/softplus.png)  

特点：
1. Softplus 是一个平滑函数，在所有点上都可微。这意味着它在反向传播时不会出现像 ReLU 那样的导数不连续问题。
2. Softplus 在整个定义域上都有非零梯度，因此在反向传播中不会出现梯度消失的问题。
3. 计算复杂。

#### Leaky ReLU函数
$$Leaky\ ReLU(x) = max(\alpha x,x)$$

图像：  
![Leaky ReLU函数图像](post/AIGC/ac_func/Leaky_ReLU.png)  

优点：
1. 解决了ReLU函数的死亡问题。
2. 同ReLU函数一样，输入为正时，不存在梯度饱和。

缺点：
1. $\alpha$值需要人为设定，不易调参，一般取0.01。
2. 有些近似线性，导致在复杂分类中效果不好。

#### PReLU函数
$$PReLU(x) = max(\alpha x,x)$$
与Leaky ReLU激活函数不同的是，PRelu激活函数负半轴的斜率参数α 是通过学习得到的，而不是手动设置的恒定值。  

各性质类似于Leaky ReLU激活函数。

#### ELU函数
$$ELU(x) = \left\{ \begin{array}{ll} \alpha(e^x-1) & x<0 \\ x & x\geq0 \end{array} \right. $$

图像：  
![ELU函数图像](post/AIGC/ac_func/ELU.png)  

优点：
1. ELU试图将激活函数的输出均值接近于零，使正常梯度更接近于单位自然梯度，从而加快学习速度。
2. ELU 在较小的输入下会饱和至负值，从而减少前向传播的变异和信息。
3. 输出是以 0 为中心，输出范围是 -1 到 1。

缺点：
1. 计算指数函数，效率低。

#### SELU函数
$$SELU(x) = \lambda\left\{ \begin{array}{ll} \alpha(e^x-1) & x<0 \\ x & x\geq0 \end{array} \right. $$

图像：  
![SELU函数图像](post/AIGC/ac_func/SELU.png)  

特点：
1. SELU 允许构建一个映射 g，其性质能够实现 SNN(自归一化神经网络)。
2. SELU函数通过调整均值和方差来实现内部的归一化，这种内部归一化比外部归一化更快，这使得网络能够更快得收敛

PS:
SNN网络激活函数的要求：
1. 负值和正值，以便控制均值；
2. 饱和区域（导数趋近于零），以便抑制更低层中较大的方差；
3. 大于 1 的斜率，以便在更低层中的方差过小时增大方差；
4. 连续曲线。

#### swish函数
$$swish(x) = x\sigma(x)$$

图像：  
![swish函数图像](post/AIGC/ac_func/swish.png)  

特点：
1. 与sigmoid函数类似，但更平滑，在优化和泛化中起了重要作用。
2. 其无界性有助于防止慢速训练期间，梯度逐渐接近 0 并导致饱和。
